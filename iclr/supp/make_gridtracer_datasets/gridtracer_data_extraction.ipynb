{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data that this script draws from can be downloaded from [here](https://figshare.com/articles/dataset/Electric_Transmission_Infrastructure_Satellite_Imagery_Dataset_for_Computer_Vision/14935434).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs19QdqKysZY",
        "outputId": "fd56b2ab-dd39-42b2-c69d-50a0e182745f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phXQsYm1JKxx",
        "outputId": "2bea3069-8969-418c-ffcb-49394621a019"
      },
      "outputs": [],
      "source": [
        "!pip install fiftyone\n",
        "!pip uninstall opencv-python -y\n",
        "!pip install opencv-python\n",
        "!pip install geopandas\n",
        "!pip install rtree\n",
        "!pip install python-dotenv\n",
        "!pip install pycountry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxc9N9LYnaW0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def downsample(img, target_size=(256,256), quiet=True):\n",
        "    '''\n",
        "    Downsamples img to target_size\n",
        "\n",
        "    Args:\n",
        "        img(np.array): original array\n",
        "        target_size(Tuple[int, int]): desired dimension\n",
        "    '''\n",
        "\n",
        "    assert type(img).__module__ == np.__name__\n",
        "    nr, nc, l = img.shape\n",
        "    tr, tc = target_size\n",
        "    shrink_factor = min(tr/nr, tc/nc) # pick the smaller shrink factor\n",
        "\n",
        "    if not quiet:\n",
        "        print(f'original width {nc}, height: {nr}')\n",
        "    img_pil = Image.fromarray(img)\n",
        "    img_pil = img_pil.resize((round(nc*shrink_factor) ,round(nr*shrink_factor)))\n",
        "    img_resized = np.array(img_pil)\n",
        "\n",
        "    nr_, nc_, l_ = img_resized.shape\n",
        "    if not quiet:\n",
        "        print(f'resized width {nc_}, height: {nr_} with factor {shrink_factor}')\n",
        "\n",
        "    return img_resized\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXN_YP_uiqXZ"
      },
      "source": [
        "### Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXefwQOsJ-wf",
        "outputId": "bd7863ba-894f-41f4-bb82-f0fbdbd7314d"
      },
      "outputs": [],
      "source": [
        "global resolutions\n",
        "\n",
        "# precomputed resolutions of raster files by region\n",
        "resolutions = {\n",
        "        'arizona': 0.1521273311113449,\n",
        "        'sudan': 0.2941356391155026,\n",
        "        'china': 0.2789294695267606,\n",
        "        'rotorua': 0.1254966590856217,\n",
        "        'brazil': 0.4787769055150278,\n",
        "        'hartford': 0.0761820560981825,\n",
        "        'mexico': 0.1522336654793647,\n",
        "        'kansas': 0.1520310125331718,\n",
        "        'clyde': 0.1535755375821943,\n",
        "        'wilmington': 0.1522415121091316,\n",
        "        'dunedin': 0.1229411434163525,\n",
        "        'gisborne': 0.1253881937589445,\n",
        "        'palmertson': 0.1255661866324052,\n",
        "        'tauranga': 0.1254967811385344,\n",
        "        }\n",
        "\n",
        "import shutil\n",
        "import fiftyone as fo\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "from osgeo import gdal\n",
        "from itertools import product\n",
        "from PIL import Image\n",
        "from shapely.geometry import Polygon\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# basically a test if this is running in colab\n",
        "if 'content' in os.getcwd():\n",
        "    os.chdir('/content/drive/MyDrive/PyPSA_Africa_images')\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "duke_path = os.environ.get('PROJECT_DUKE_IMAGES')\n",
        "datasets_path = os.environ.get('PROJECT_DATASETS')\n",
        "root_path = os.environ.get('PROJECT_ROOT')\n",
        "sys.path.append(root_path)\n",
        "\n",
        "from src.utils.dataset_utils import fix_annots, fix_filenames\n",
        "from src.utils.image_utils import downsample\n",
        "\n",
        "assert duke_path is not None, f'Could not locate .env file. Got duke_path {duke_path}'\n",
        "assert datasets_path is not None, f'Could not locate .env file. Got datasets_path {datasets_path}'\n",
        "\n",
        "def extract_duke_dataset(dirs, \n",
        "                         target_base_dir,\n",
        "                         size=512, \n",
        "                         base_path=\"\", \n",
        "                         train_ratio=0.8,\n",
        "                         target_resolution=None,\n",
        "                         bbox_threshold=None,\n",
        "                         tower_types=['DT', 'TT', 'OT'],\n",
        "                         ):\n",
        "    \"\"\"\n",
        "    Extracts training images and bounding from region-zips provided in\n",
        "    'https://figshare.com/articles/dataset/Electric_Transmission_and_\n",
        "    Distribution_Infrastructure_Imagery_Dataset/6931088'\n",
        "    \n",
        "    Iterates over a list of directories and creates examples if it encounters the\n",
        "    following structure in these directories:\n",
        "        [dir_name]/raw/[.tif, .csv, .geojson etc. files]\n",
        "        stores images to the following structure\n",
        "        [dir_name]/examples/[prefix]+[id]+\".png\"]\n",
        "        and a summarizing geojson file storing a dataframe of filenames and bbox\n",
        "        [dir_name]/examples/[prefix]+\"examples.geojson\"\n",
        "    \n",
        "    Data must be unzipped!\n",
        "    Be sure that all directories in [dirs] are in os.getcwd()!\n",
        "    \n",
        "    ----------\n",
        "    Arguments:\n",
        "        dirs : (list of str)\n",
        "            list with names of directories which satisfy the outlined structure\n",
        "        target_base_dir (str):\n",
        "            directory where resulting datasets should be stored\n",
        "        size : (int)\n",
        "            width and height of resulting example-images (images are made quadratic)\n",
        "        base_path : (str)\n",
        "            path to directories from which all dirs are accessible\n",
        "        train_ratio : (float)\n",
        "            share of examples labelled as part of training set (rest is val set),\n",
        "        target_resolution: (None or float)\n",
        "            if float, resolutions are scaled to target_resolution\n",
        "        bbox_threshold(None or float):\n",
        "            if float, only towers are included that have bboxes which both for height and width are\n",
        "            height \n",
        "        tower_types(List[str]):\n",
        "            list of tower types that are taken into the dataset\n",
        "                TT: transmission towers\n",
        "                DT: distribution towers\n",
        "                TT: other towers\n",
        "        \n",
        "    ----------\n",
        "    Returns:\n",
        "        -\n",
        "    \"\"\"\n",
        "\n",
        "    prefixes = [word[:2].upper() for word in dirs]\n",
        "\n",
        "    width, height = size, size\n",
        "\n",
        "    if bbox_threshold is None:\n",
        "        bbox_threshold = 0.\n",
        "    \n",
        "    label = 'tower'\n",
        "\n",
        "    out_train = 'train'\n",
        "    out_val = 'val' \n",
        "\n",
        "    print('Starting dataset extraction')\n",
        "    print('Note currently all towers are labelled as tower')\n",
        "\n",
        "    if not os.path.isdir(target_base_dir): \n",
        "        os.mkdir(target_base_dir)\n",
        "\n",
        "    for country, prefix in zip(dirs, prefixes):\n",
        "        \n",
        "        # set up working path\n",
        "        print(\"Extracting images from {}...\".format(country))\n",
        "\n",
        "        print(f'Assuming resolution {resolutions[country]} m/pixel')\n",
        "        res = resolutions[country]\n",
        "\n",
        "        country_out = os.path.join(target_base_dir, country)\n",
        "        if not os.path.isdir(country_out): \n",
        "            os.mkdir(country_out)\n",
        "\n",
        "        train_path = os.path.join(country_out, out_train)\n",
        "        val_path = os.path.join(country_out, out_val)\n",
        "\n",
        "        if not os.path.isdir(train_path): \n",
        "            os.mkdir(train_path)\n",
        "        if not os.path.isdir(val_path): \n",
        "            os.mkdir(val_path)\n",
        "\n",
        "        # train_path = out_train\n",
        "\n",
        "        os.chdir(os.path.join(os.getcwd(), country, \"raw\"))\n",
        "        \n",
        "        # setup directory for resulting images\n",
        "        # set up resulting dataset of examples (with towers)\n",
        "        tower_df = gpd.GeoDataFrame({\"filename\": [], \n",
        "                                \"ul_x\": [], \"ul_y\": [], \"lr_x\": [], \"lr_y\": [], \n",
        "                                })\n",
        "        \n",
        "        # set up datasets for current country\n",
        "        try: \n",
        "            dataset_train = fo.Dataset(name=country+'_'+out_train)\n",
        "        except:\n",
        "            dataset_train = fo.load_dataset(country+'_'+out_train)\n",
        "            dataset_train.delete()\n",
        "            dataset_train = fo.Dataset(name=country+'_'+out_train)\n",
        "        dataset_train.persistent = False\n",
        "\n",
        "        try: \n",
        "            dataset_val = fo.Dataset(name=country+'_'+out_val)\n",
        "        except:\n",
        "            dataset_val = fo.load_dataset(country+'_'+out_val)\n",
        "            dataset_val.delete()\n",
        "            dataset_val = fo.Dataset(name=country+'_'+out_val)\n",
        "        dataset_val.persistent = False\n",
        "        \n",
        "        # Starting with adding examples to the training set\n",
        "        curr_path = train_path\n",
        "        curr_dataset = dataset_train\n",
        "        switched_already = False\n",
        "\n",
        "        # create list of relevant files\n",
        "        filelist = os.listdir()\n",
        "        csv_files = [fn for fn in filelist if fn.endswith('.csv')]\n",
        "\n",
        "        unders = [i for i, letter in enumerate(csv_files[0]) if letter is \"_\"]\n",
        "        \n",
        "        file_prefix = csv_files[0][:unders[-1]+1]\n",
        "        num_files = len(csv_files)\n",
        "\n",
        "        tif_files = [file_prefix + str(i+1) + '.tif' for i in range(num_files)]  \n",
        "        geojson_files = [file_prefix + str(i+1) + '.geojson' for i in range(num_files)]  \n",
        "\n",
        "        if target_resolution is not None:\n",
        "            scaled_size = int(size * target_resolution / res)\n",
        "            print(f'Pictures from {country} will have {scaled_size}x{scaled_size} pixels.')\n",
        "            scaled_width, scaled_height = scaled_size, scaled_size\n",
        "        else:\n",
        "            scaled_height, scaled_width = height, width\n",
        "        \n",
        "        # iterate over files\n",
        "        for i, (tif, geojson) in enumerate(zip(tif_files, geojson_files)):        \n",
        "\n",
        "            if (i+1) / num_files > train_ratio and not switched_already: \n",
        "                print('Switching to mode val after {} of {} files due to train ratio'.format(\n",
        "                      i, num_files, train_ratio, train_ratio))\n",
        "        \n",
        "                print(base_path, country, curr_path)\n",
        "                export_dir = curr_path\n",
        "\n",
        "                # Export training dataset\n",
        "                if len(curr_dataset) > 0:\n",
        "                    curr_dataset.export(\n",
        "                        export_dir=export_dir,\n",
        "                        dataset_type=fo.types.COCODetectionDataset,\n",
        "                        label_field='ground_truth',\n",
        "                        )\n",
        "                else:\n",
        "                    print(f'Did not export empty dataset for training in {country}')\n",
        "                \n",
        "                curr_path = val_path\n",
        "                curr_dataset = dataset_val\n",
        "                switched_already = True\n",
        "\n",
        "\n",
        "            print(\"Opening geojson file: \", geojson)\n",
        "            # open files and get bands\n",
        "            try:\n",
        "                annots = gpd.read_file(geojson)\n",
        "            except:\n",
        "                print(\"Unable to read annotation file {}\".format(geojson))\n",
        "                print(\"Continuing to the next file...\")\n",
        "                continue\n",
        "\n",
        "            # make sure geojson contains information\n",
        "            if len(annots.columns) == 1:\n",
        "                print('Bad geojson detected! Continuing...') \n",
        "                continue\n",
        "\n",
        "            ds = gdal.Open(tif)\n",
        "            bands = [ds.GetRasterBand(i) for i in range(1, 4)]\n",
        "            info = gdal.Info(tif, format=\"json\")\n",
        "\n",
        "            pd.set_option('display.max_columns', None)\n",
        "\n",
        "            # remove all assets except towers            \n",
        "            remove_assets = [\"DL\", \"TL\", \"OL\", \"SS\"]\n",
        "            for to_remove in remove_assets:\n",
        "                annots = annots[annots[\"label\"] != to_remove]\n",
        "\n",
        "            def to_pixels(geom):\n",
        "                '''\n",
        "                receives pixel coordinates as string and returns columns \n",
        "                upper left, lower right and geometry as Polygon (rectangular) \n",
        "                all coordinates are relative to the tif file the assets is in\n",
        "                '''\n",
        "                geom = geom.split(\" \")\n",
        "                geom.remove('[')\n",
        "                geom.remove(']')\n",
        "\n",
        "                # transform to Polygon with rectangular bbox\n",
        "                geom = [entry for entry in geom if not '[' in entry and not ']' in entry]\n",
        "                geom = [int(float(entry.replace(\",\", \"\"))) for entry in geom]\n",
        "                x, y = geom[::2], geom[1::2] \n",
        "                geom = Polygon([[max(x), max(y)], [max(x), min(y)], [min(x), min(y)], [min(x), max(y)]])\n",
        "                return np.array([min(x), min(y)]), np.array([max(x), max(y)]), geom\n",
        "\n",
        "            # make sure the dataframe contains only towers\n",
        "            annots = annots[annots['geometry'].apply(lambda x: isinstance(x, Polygon))]\n",
        "            if annots.empty: continue\n",
        "\n",
        "            annots[\"ul\"], annots[\"lr\"], annots['geometry'] = zip(*annots['pixel_coordinates'].map(to_pixels))\n",
        "\n",
        "            tif_width, tif_height = info['size'][0], info['size'][1]\n",
        "\n",
        "            for curr, tower in annots.iterrows():\n",
        "\n",
        "                if not isinstance(tower.geometry, Polygon): continue\n",
        "                \n",
        "                if not tower['label'] in tower_types:\n",
        "                    continue\n",
        "\n",
        "                example_name = prefix + '_' + str(np.random.randint(1e10, 1e11)) + '.png'\n",
        "\n",
        "                # define the bounds of random offset\n",
        "                bb_ul, bb_lr = tower['ul'], tower['lr']\n",
        "                min_x, max_x = max(0, bb_lr[0] - scaled_width), min(bb_ul[0], tif_width - scaled_width)\n",
        "                min_y, max_y = max(0, bb_lr[1] - scaled_height), min(bb_ul[1], tif_height - scaled_height)\n",
        "\n",
        "                # randomly draw corner of image (this can fail if towers are close to the frame -> skip tower)\n",
        "                try:\n",
        "                    img_ul_x = np.random.randint(min_x, max_x)\n",
        "                    img_ul_y = np.random.randint(min_y, max_y)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                # determine bounding box relative to new image\n",
        "                bb_ul -= np.array([img_ul_x, img_ul_y])\n",
        "                bb_lr -= np.array([img_ul_x, img_ul_y])\n",
        "\n",
        "                # add main tower in image \n",
        "                outer_bbox = [bb_ul[0], bb_ul[1], bb_lr[0]-bb_ul[0], bb_lr[1]-bb_ul[1]]\n",
        "                outer_bbox = (np.array(outer_bbox) / scaled_width).tolist()\n",
        "\n",
        "                if outer_bbox[2] < bbox_threshold or outer_bbox[3] < bbox_threshold:\n",
        "                    continue\n",
        "\n",
        "                # set up image and new filename\n",
        "                new_img = np.zeros((scaled_height, scaled_width, 3), dtype=np.uint8)\n",
        "        \n",
        "                # transfer pixel data\n",
        "                try:\n",
        "                    for i in range(3):\n",
        "                        new_img[:,:,i] = bands[i].ReadAsArray(img_ul_x, img_ul_y, scaled_width, scaled_height)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                # transform array to image\n",
        "                new_img = downsample(new_img, target_size=(size, size))\n",
        "\n",
        "                img = Image.fromarray(new_img, 'RGB')\n",
        "                img.save(os.path.join('./../', curr_path, example_name), quality=100)\n",
        "\n",
        "                # add to dataset\n",
        "                sample = fo.Sample(filepath=os.path.join(\n",
        "                                   base_path, country, curr_path, example_name)\n",
        "                                   )\n",
        "                \n",
        "                detections = []\n",
        "                detections.append(fo.Detection(label=label, bounding_box=outer_bbox))\n",
        "\n",
        "                # create Polygon of created image\n",
        "                img_corner = np.array([img_ul_x, img_ul_y])\n",
        "                img_polygon = Polygon([\n",
        "                                    img_corner,\n",
        "                                    img_corner + np.array([scaled_width, 0]),\n",
        "                                    img_corner + np.array([scaled_width, scaled_height]),\n",
        "                                    img_corner + np.array([0, scaled_height])\n",
        "                                    ])\n",
        "\n",
        "                # add secondary towers that happen to be in the same image\n",
        "                for j, other in annots.iterrows():\n",
        "\n",
        "                    if tower['geometry'] == other['geometry']:\n",
        "                        continue\n",
        "\n",
        "                    if not other['label'] in tower_types:\n",
        "                        continue\n",
        "\n",
        "                    #if img_polygon.contains(other[\"geometry\"]):\n",
        "                    if img_polygon.intersects(other[\"geometry\"]):\n",
        "\n",
        "                        ul_pixels = np.min(other['geometry'].exterior.xy, axis=1)\n",
        "                        lr_pixels = np.max(other['geometry'].exterior.xy, axis=1)\n",
        "\n",
        "                        ul = (ul_pixels - img_corner) / scaled_width\n",
        "                        lr = (lr_pixels - img_corner) / scaled_width\n",
        "                        w, h = lr - ul\n",
        "\n",
        "                        bbox = [ul[0], ul[1], w, h]\n",
        "\n",
        "                        if not img_polygon.contains(other['geometry']):\n",
        "                            in_part = other['geometry'].intersection(img_polygon)\n",
        "                            shared_fraction = in_part.area / other['geometry'].area\n",
        "\n",
        "                            bbox[0] = max(bbox[0], 0)\n",
        "                            bbox[1] = max(bbox[1], 0)\n",
        "                            bbox[2] = min(bbox[2], 1 - bbox[0])\n",
        "                            bbox[3] = min(bbox[3], 1 - bbox[1])\n",
        "                        \n",
        "                        else:\n",
        "                            shared_fraction = 1\n",
        "\n",
        "                        if bbox[2] < bbox_threshold or bbox[3] < bbox_threshold:\n",
        "                            continue\n",
        "\n",
        "                        if not bbox == outer_bbox and shared_fraction > 0.5:\n",
        "                            detections.append(fo.Detection(label=label, bounding_box=bbox))\n",
        "                \n",
        "                sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
        "                \n",
        "                curr_dataset.add_sample(sample)\n",
        "                \n",
        "                \n",
        "        export_dir = curr_path\n",
        "        label_field = \"ground_truth\"  \n",
        "\n",
        "        # Export training dataset\n",
        "        try:\n",
        "            curr_dataset.export(\n",
        "                    export_dir=export_dir,\n",
        "                    dataset_type=fo.types.COCODetectionDataset,\n",
        "                    label_field='ground_truth',\n",
        "                    )\n",
        "        except ValueError:\n",
        "            print(f'Could not export: {export_dir}; Length: {len(curr_dataset)}')\n",
        "            print('Continuing...')\n",
        "            \n",
        "        fix_filenames(os.path.join(base_path, country, out_val, 'labels.json'))\n",
        "\n",
        "        os.chdir(os.path.abspath(os.path.join('', '../..')))\n",
        "\n",
        "    print('Done with all regions. Proceeding to packaging...')\n",
        "    ds_name = target_base_dir.split('/')[-1]\n",
        "\n",
        "    for mode in ['train', 'val']:\n",
        "\n",
        "        datasets = []\n",
        "        export_dir = os.path.join(target_base_dir, ds_name+'_'+mode)\n",
        "\n",
        "        for country in dirs:\n",
        "\n",
        "            ds_path = os.path.join(target_base_dir, country, mode)\n",
        "\n",
        "            if not os.path.isdir(ds_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                datasets.append(fo.Dataset.from_dir(\n",
        "                        dataset_type=fo.types.COCODetectionDataset,\n",
        "                            data_path=os.path.join(ds_path, 'data'),\n",
        "                            labels_path=os.path.join(ds_path, 'labels.json')))\n",
        "            except ValueError:\n",
        "                continue \n",
        "\n",
        "        if len(datasets) == 0:\n",
        "            print(f'Could not create dataset in mode {mode} for {ds_name}')\n",
        "            continue\n",
        "         \n",
        "        ds = datasets[0]\n",
        "        for curr_ds in datasets[1:]:\n",
        "            ds.merge_samples(curr_ds) \n",
        "\n",
        "        print('Exporting...')\n",
        "        ds.export(\n",
        "                    export_dir=export_dir,\n",
        "                    dataset_type=fo.types.COCODetectionDataset,\n",
        "                    label_field='ground_truth',\n",
        "                    )\n",
        "        print(f'Saved dataset to {export_dir}')\n",
        "\n",
        "        fix_annots(os.path.join(export_dir, 'labels.json'))\n",
        "\n",
        "    print('Done with merging dataset - only cleanup remaining')\n",
        "\n",
        "    for country in dirs:\n",
        "        \n",
        "        to_delete = os.path.join(target_base_dir, country)\n",
        "        if os.path.isdir(to_delete):\n",
        "            shutil.rmtree(to_delete)\n",
        "\n",
        "    print('Prepared datasets!')\n",
        "    train_path = os.path.join(target_base_dir, ds_name+'_train')\n",
        "    val_path = os.path.join(target_base_dir, ds_name+'_val')\n",
        "    print(f'Find training set at: {train_path}')\n",
        "    print(f'Find validation set at: {val_path}')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    dataset_name = 'testset'\n",
        "    target_base_dir = os.path.join(datasets_path, dataset_name)\n",
        "    os.chdir(duke_path)\n",
        "    dirs = [ \n",
        "            #'hartford',   #  (APPEARS TO HAVE CORRUPTED GEOJSON FILES)\n",
        "             'china',\n",
        "             'kansas',\n",
        "             'dunedin',\n",
        "             'gisborne',\n",
        "             'palmertson',\n",
        "             'rotorua',\n",
        "             'tauranga',\n",
        "             'wilmington',\n",
        "             'arizona',\n",
        "             'clyde',\n",
        "             'sudan',\n",
        "             'mexico',\n",
        "             'brazil',\n",
        "            ]\n",
        "\n",
        "    extract_duke_dataset(dirs, \n",
        "                         target_base_dir, \n",
        "                         size=512,\n",
        "                         train_ratio=0.8,\n",
        "                         base_path=duke_path,\n",
        "                         target_resolution=0.35,\n",
        "                         # bbox_threshold=0.025,\n",
        "                         tower_types=['TT']\n",
        "                        )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "ffdb0c89cc842367a93be6eb468afe23766ac3749a570c7a1ecbb8f757cdd35d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
