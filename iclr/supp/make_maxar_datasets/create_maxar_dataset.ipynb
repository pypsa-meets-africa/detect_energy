{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkiMWXdcKq7Q",
        "outputId": "f4c85472-537f-478a-f8b9-279e4529ce41"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BECiMIJiLAGW",
        "outputId": "8acdbe83-e3a3-48ab-cc5d-869ceef93fee"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas rtree\n",
        "!pip install fiftyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buexhyamLDBb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import importlib\n",
        "import sys\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import fiftyone as fo\n",
        "import json\n",
        "\n",
        "import create_coco_format\n",
        "import filter_tiles"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sllIuiOkYAo",
        "outputId": "2ab5039e-1a06-4db2-acf7-c2bc5873e180"
      },
      "outputs": [],
      "source": [
        "def fix_annots(file):\n",
        "    '''\n",
        "    adds information on the 'iscrowd' property \n",
        "    for training with detectron2 to an annotation json file\n",
        "    made by fiftyone\n",
        "    '''\n",
        "    dictionary = json.load(open(file)) \n",
        "    for annot in dictionary['annotations']:\n",
        "        annot['iscrowd'] = 0\n",
        "    \n",
        "    with open(file, \"w\") as f:\n",
        "        json.dump(dictionary, f)\n",
        "\n",
        "\n",
        "def create_maxar_dataset(country, base, out_dir, img_dir, gdf_file, max_img=1000):\n",
        "\n",
        "    print('Assuming image with 512 and heigth 512.')\n",
        "    width = 512\n",
        "    height = 512\n",
        "\n",
        "    os.mkdir(out_dir)\n",
        "\n",
        "    df = gpd.read_file(gdf_file) \n",
        "    print(df.head())\n",
        "\n",
        "    # set up datasets for current country\n",
        "    try: \n",
        "        dataset = fo.Dataset(name=country+'_val')\n",
        "    except:\n",
        "        dataset = fo.load_dataset(country+'_val')\n",
        "        dataset.delete()\n",
        "        dataset = fo.Dataset(name=country+'_val')\n",
        "    dataset.persistent = False\n",
        "\n",
        "    label_field = 'ground_truth'\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "\n",
        "        if i == max_img:\n",
        "            break\n",
        "\n",
        "        example_name = row.filename\n",
        "\n",
        "        sample = fo.Sample(filepath=os.path.join(img_dir, example_name))\n",
        "        bbox = [\n",
        "                row.ul_x / width,\n",
        "                row.ul_y / height,\n",
        "                (row.lr_x - row.ul_x) / width,\n",
        "                (row.lr_y - row.ul_y) / height,\n",
        "                ]\n",
        "        detections = list()\n",
        "        detections.append(fo.Detection(label='tower', bounding_box=bbox))\n",
        "\n",
        "        sample['ground_truth'] = fo.Detections(detections=detections)\n",
        "        dataset.add_sample(sample)\n",
        "\n",
        "    dataset.export(\n",
        "        export_dir=out_dir,\n",
        "        dataset_type=fo.types.COCODetectionDataset,\n",
        "        label_field=label_field,\n",
        "       )\n",
        "    \n",
        "\n",
        "country = 'bangladesh'\n",
        "tower_gdf_file = 'BD_tower_examples.geojson'\n",
        "base = ...\n",
        "\n",
        "out_dir = os.path.join(base, 'datasets', country+'_val/')\n",
        "img_dir = os.path.join(base, 'maxar', country, 'images_512')\n",
        "gdf_file = os.path.join(img_dir, tower_gdf_file)\n",
        "\n",
        "create_maxar_dataset(country, base, out_dir, img_dir, gdf_file, max_img=300000)\n",
        "fix_annots(os.path.join(out_dir, 'labels.json'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "pK29P96Z2osb",
        "outputId": "cbd7b3c9-67d0-439e-cadf-ea7b7fa8844e"
      },
      "outputs": [],
      "source": [
        "dataset = fo.Dataset.from_dir(\n",
        "            dataset_type=fo.types.COCODetectionDataset,\n",
        "            data_path=out_dir+'data/',\n",
        "            labels_path=out_dir+'labels.json',)\n",
        "\n",
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u_8HFm7YqaS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from osgeo import osr, gdal\n",
        "import numpy as np\n",
        "from gdalconst import *\n",
        "from shapely.geometry import Polygon\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "import os\n",
        "import rtree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#%%\n",
        "def make_polygon_list(path, resume_file=None):\n",
        "    \"\"\"\n",
        "    Creates GeoDataFrame, checks the tif files in a desired dir \n",
        "    and adds their geometries (with respective file name) as row\n",
        "    to GeoDataFrame\n",
        "    -----------\n",
        "    Arguments:\n",
        "    path : (str)\n",
        "        directory with tif files\n",
        "    resume_file : (str or None)\n",
        "        if None: initializes net GeoDataFrame\n",
        "        otherwise loads (and resumes from) existing one\n",
        "    \"\"\"\n",
        "\n",
        "    tif_files = []\n",
        "\n",
        "    # iterate over all .tif files in desired dir\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".tif\"): \n",
        "            tif_files.append(os.path.join(path, filename))\n",
        "\n",
        "    # add data to existing dataframe if desired\n",
        "    try:\n",
        "        coverage = gpd.read_file(resume_file)\n",
        "        print(f\"Adding coverage to {resume_file}\")\n",
        "    except:\n",
        "        coverage = gpd.GeoDataFrame({\"filename\": [], \"geometry\": []})\n",
        "\n",
        "    # extract geometries of .tif files and add to geodataframe\n",
        "    for file in tif_files:\n",
        "        try:\n",
        "            info = gdal.Info(file, format=\"json\")\n",
        "        except SystemError:\n",
        "            print(f'Skipping raster {file}')\n",
        "            continue\n",
        "        corners = info[\"cornerCoordinates\"]\n",
        "        poly = Polygon((\n",
        "                    corners[\"upperLeft\"],\n",
        "                    corners[\"upperRight\"],\n",
        "                    corners[\"lowerRight\"],\n",
        "                    corners[\"lowerLeft\"]\n",
        "                        ))\n",
        "        coverage = coverage.append({\"filename\": file, \"geometry\": poly}, ignore_index=True)\n",
        "\n",
        "    coverage = coverage.set_crs(epsg=4326)\n",
        "\n",
        "    return coverage\n",
        "\n",
        "def make_examples(assets,\n",
        "                  coverage,\n",
        "                  img_path=\"/../examples/\",\n",
        "                  max_length=10,\n",
        "                  height=512,\n",
        "                  width=512,\n",
        "                  bbox_height=25,\n",
        "                  bbox_width=30,\n",
        "                  random_offset = True,\n",
        "                  seed = None,\n",
        "                  examples_per_tower=1):\n",
        "    \"\"\"\n",
        "    Expects dataframe of energy infrastructure assets in fn. Iterates over df\n",
        "    and for each asset finds the respective polygon from coverage geodataframe.\n",
        "    Then opens the .tif file that corresponds to the polygon and cuts out the\n",
        "    respective pixels. The pixels are turned into a png files and saves.\n",
        "    Additionally, the respective file name, geometry, bbox, is written into\n",
        "    a new GeoDataFrame\n",
        "    ----------\n",
        "    Arguments:\n",
        "    assets : (str or GeoDataFrame)\n",
        "        path to or GeoDataFrame itself of energy assets\n",
        "    coverage : (str or GeoDataFrame)\n",
        "        path to or GeoDataFrame itself of geometries satellite imagery\n",
        "    img_path : (str)\n",
        "        path to directory where resulting examples will vbe stored\n",
        "    max_length : (int / None)\n",
        "        restricts number of images created if desired\n",
        "    height : (int)\n",
        "        number of pixels in y direction\n",
        "    width : (int)\n",
        "        number of pixels in x direction\n",
        "    random_offset : (bool)\n",
        "        set false for zero offset otherwise randomly set\n",
        "    seed : (int)\n",
        "        seed value for numpy random generator\n",
        "    examples_per_tower : (int)\n",
        "        number of examples generated per tower\n",
        "    ----------\n",
        "    Returns:\n",
        "    dataset : (GeoDataFrame)\n",
        "        df of created examples. contains for each image:\n",
        "            filename (ending in .png)\n",
        "            for bbox: upperleft and lowerright\n",
        "            geometry as Polygon\n",
        "    Also\n",
        "    Saves created examples as .png files to img_path\n",
        "    In the same directory the GeoJSON of the respective\n",
        "    GeoDataFrame dataset is stored in the same directory\n",
        "    \"\"\"\n",
        "\n",
        "    # img_path = os.path.abspath(\"\") + img_path\n",
        "\n",
        "    if isinstance(assets, str): assets = gpd.read_file(assets)\n",
        "    if isinstance(coverage, str): coverage = gpd.read_file(coverage)\n",
        "\n",
        "    # set up resulting dataset of examples (with towers)\n",
        "    dataset = gpd.GeoDataFrame({\"filename\": [],\n",
        "                                \"ul_x\": [], \"ul_y\": [], \"lr_x\": [], \"lr_y\": [],\n",
        "                                \"geometry\": []}).set_crs(epsg=4326)\n",
        "\n",
        "    assets = gpd.sjoin(assets, coverage, how=\"inner\").set_crs(epsg=4326)\n",
        "    assets = assets.drop([\"index_right\"], axis=1)\n",
        "\n",
        "    # for image labeling\n",
        "    pos = [i for i, sign in enumerate(img_path) if sign is '/'][-1]\n",
        "    prefix = img_path[pos+1:]\n",
        "\n",
        "    print(f\"Maxiumum Number of Examples: {len(assets)}\")\n",
        "    if len(assets)<max_length:\n",
        "        max_length = len(assets)\n",
        "    \n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # iterate over .tif files\n",
        "    for i, image in enumerate(coverage[\"filename\"]):\n",
        "\n",
        "        # open files\n",
        "        ds = gdal.Open(image)\n",
        "        info = gdal.Info(image, format=\"json\")\n",
        "        bands = [ds.GetRasterBand(i) for i in range(1, 4)]\n",
        "\n",
        "        # extract relevant geographical data\n",
        "        transform = info[\"geoTransform\"]\n",
        "        upper_left = np.array([transform[0], transform[3]])\n",
        "        pixel_size = np.array([transform[1], transform[5]])\n",
        "\n",
        "        # iterate over assets in that image\n",
        "        for row in assets[assets[\"filename\"] == image].itertuples():\n",
        "\n",
        "            # (default=1) generates multiple examples for every examples\n",
        "            for j in range(examples_per_tower):\n",
        "\n",
        "                # compute relevant pixels\n",
        "                p = row.geometry\n",
        "                coords = np.array([p.xy[0][0], p.xy[1][0]])\n",
        "                pixels = np.around((coords - upper_left) / pixel_size)\n",
        "                pixels -= np.array([width // 2, height // 2])\n",
        "                # add random offset (8 is minimal distance to image boundary)\n",
        "\n",
        "                offset = np.zeros(2)\n",
        "                if random_offset is True:\n",
        "                    offset[0] = np.random.randint(-width//2 + 20, width//2 - 20)\n",
        "                    offset[1] = np.random.randint(-height//2 + 20, height//2 - 20)\n",
        "\n",
        "                pixels -= offset\n",
        "                x, y = int(pixels[0]), int(pixels[1])\n",
        "\n",
        "                # set up image and new filename\n",
        "                filename = str(int(10*row.id+j))\n",
        "                new_img = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "\n",
        "                # transfer pixel data\n",
        "                try:\n",
        "                    for i in range(3):\n",
        "                        new_img[:,:,i] = bands[i].ReadAsArray(x, y, width, height)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                # create Polygon of created image\n",
        "                img_corner = upper_left + pixels*pixel_size\n",
        "                img_polygon = Polygon([\n",
        "                                       img_corner,\n",
        "                                       img_corner + pixel_size*np.array([width,0]),\n",
        "                                       img_corner + pixel_size*np.array([width,height]),\n",
        "                                       img_corner + pixel_size*np.array([0,height])\n",
        "                                      ])\n",
        "\n",
        "                # get pixels of bbox; format: (ul_x, ul_y, lr_x, lr_y)\n",
        "                ul = np.array([width//2, height//2]) + offset\n",
        "                bbox = [\n",
        "                        ul[0] - bbox_width // 2,\n",
        "                        ul[1] - bbox_height // 2,\n",
        "                        ul[0] + bbox_width // 2,\n",
        "                        ul[1] + bbox_height // 2\n",
        "                ]\n",
        "\n",
        "                # do not include images that are contain blank spots\n",
        "                black_threshold = 10\n",
        "                binarr = np.where(new_img < black_threshold, 1, 0) #black_point is upper limit\n",
        "                # Find Total sum of 2D array thresh\n",
        "                total = sum(map(sum, binarr))\n",
        "                ratio = total/height/width\n",
        "\n",
        "                if (ratio > 0.5).sum() == 3:\n",
        "                    print('filtered by black borders!')\n",
        "                    continue\n",
        "\n",
        "                # do not include images that contain clouds\n",
        "                white_point = 180\n",
        "                # Put threshold to make it binary\n",
        "                binarr = np.where(new_img>white_point, 1, 0) #white point is lower limit\n",
        "                # Find Total sum of 2D array thresh\n",
        "                total = sum(map(sum, binarr))\n",
        "                ratio = total/height/width\n",
        "                if (ratio > 0.45).sum() == 3:\n",
        "                    print('filtered by cloudy!')\n",
        "                    continue\n",
        "\n",
        "                # exclude images that are blurry \n",
        "                blurr_threshold = 0.65\n",
        "                # _, s, _ = np.linalg.svd(new_img)        \n",
        "                _, s, _ = np.linalg.svd(new_img.sum(axis=2))        \n",
        "                sv_num = new_img.shape[0] // 50\n",
        "                ratio = s[:sv_num].sum() / s.sum()\n",
        "                if ratio > blurr_threshold and not 'australia' in image:\n",
        "                    print('filtered by blurry!')\n",
        "                    continue\n",
        "\n",
        "                # transform array to image\n",
        "                img = Image.fromarray(new_img, 'RGB')\n",
        "                img.save(img_path + filename + \".png\", quality=100)\n",
        "\n",
        "                # add resulting image to dataset\n",
        "                dataset = dataset.append({\"filename\": prefix + filename + '.png',\n",
        "                                          \"ul_x\": bbox[0],\n",
        "                                          \"ul_y\": bbox[1],\n",
        "                                          \"lr_x\": bbox[2],\n",
        "                                          \"lr_y\": bbox[3],\n",
        "                                          \"geometry\": img_polygon}, \n",
        "                                          ignore_index=True)\n",
        "\n",
        "                if len(dataset)%50 == 0:\n",
        "                    print(\"Created {} Examples!\".format(len(dataset)))\n",
        "\n",
        "\n",
        "                # early stoppage\n",
        "                if len(dataset) == max_length:\n",
        "                    prev_len = len(dataset)\n",
        "                    dataset.drop_duplicates(subset=['filename'], inplace=True) # For assets that exitst in overlapping coverage areas\n",
        "                    new_len = len(dataset)\n",
        "                    if new_len < prev_len:\n",
        "                        print(f\"removed {prev_len - new_len} duplicates\")\n",
        "                        print(f\"remaining {new_len} examples\")\n",
        "                    dataset.to_file(img_path + \"tower_examples.geojson\", driver=\"GeoJSON\")\n",
        "                    return None\n",
        "                \n",
        "    dataset.to_file(img_path + \"tower_examples.geojson\", driver=\"GeoJSON\")            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4kkldYWKgCH",
        "outputId": "aaff7693-2baf-44ad-fa15-a3dc8bfa1b15"
      },
      "outputs": [],
      "source": [
        "dirs = [\n",
        "        'australia',\n",
        "        'bangladesh',\n",
        "        ]\n",
        "country_dict = {'sierra_leone': 'SL', 'ghana':'GH', 'malawi': 'MW', \n",
        "                    'drc': 'CD', 'australia': 'AU', 'bangladesh': 'BD'}\n",
        "base_path = ...\n",
        "\n",
        "for country in dirs:\n",
        "    country_code = country_dict[country]\n",
        "\n",
        "    out_path = os.path.join(base_path, 'maxar', country, 'images_512')\n",
        "    \n",
        "    max_ratio = 1.\n",
        "    print(f\"Making Examples for {country}!\")\n",
        "  \n",
        "    # assess satellite imagery\n",
        "\n",
        "    data_path = os.path.join(base_path, 'maxar', country, 'raw')\n",
        "\n",
        "    coverage_polygon = make_polygon_list(data_path)\n",
        "    towers_file = os.path.join(data_path, f\"{country_code}_raw_towers.geojson\")\n",
        "  \n",
        "    tower_df = gpd.read_file(towers_file)\n",
        "    num_towers = len(tower_df)\n",
        "    max_towers = int(max_ratio*num_towers)\n",
        "  \n",
        "    # creates the examples\n",
        "    make_examples(towers_file,\n",
        "                  coverage_polygon,\n",
        "                  img_path=os.path.join(out_path, f\"{country_code}_\"), \n",
        "                  max_length=max_towers,\n",
        "                  height=512, \n",
        "                  width=512,\n",
        "                  bbox_height=50,\n",
        "                  bbox_width=60,\n",
        "                  random_offset=True, \n",
        "                  seed=2021,\n",
        "                  )\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tUYbHCbvGfa"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "\n",
        "def vis_image(examples, path, show_box=True, idx=0):\n",
        "    \"\"\"\n",
        "    Opens image of desired index by reading the filename from examples\n",
        "    Shows the image, (with bounding box if desired)\n",
        "    ----------\n",
        "    Arguments:\n",
        "    examples : (GeoDataFrame or str)\n",
        "        gdf of all examples - used to obtain filename and bbox\n",
        "    path : (str)\n",
        "        to directory of images and dataframe\n",
        "    show_box : (bool)\n",
        "        bounding box is shown if True\n",
        "    idx : (int)\n",
        "        desired example\n",
        "    \n",
        "    ----------\n",
        "    Returns:\n",
        "    -\n",
        "    \"\"\" \n",
        "\n",
        "    if isinstance(examples, str): examples = gpd.read_file(path + examples)\n",
        "\n",
        "    example = examples.iloc[idx]\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    img = plt.imread(path + example.filename)\n",
        "    fig = plt.imshow(img)\n",
        "\n",
        "    if show_box:\n",
        "        bbox = [example.ul_x, example.ul_y, example.lr_x, example.lr_y]\n",
        "        rect = plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2] - bbox[0],\n",
        "                             height=bbox[3] - bbox[1], fill=False,\n",
        "                             edgecolor=\"r\", linewidth=2)\n",
        "        fig.axes.add_patch(rect)\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwDLgNaCuB6V"
      },
      "outputs": [],
      "source": [
        "\n",
        "country = \"MW\"\n",
        "gjson_path = country + '_tower_examples.geojson'\n",
        "\n",
        "path = os.path.join(base_path, 'MAXAR_256/')\n",
        "examples = gpd.read_file(os.path.join(path, gjson_path))\n",
        "\n",
        "\n",
        "for _ in range(60):\n",
        "    num_ex = np.random.randint(0, 300)\n",
        "    path = os.path.join(base_path, 'MAXAR_256/')\n",
        "    vis_image(gjson_path, path, idx=num_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW3txhLphJVn"
      },
      "outputs": [],
      "source": [
        "merged_df = gpd.GeoDataFrame()\n",
        "example_path = os.path.join(base_path, 'MAXAR_256')\n",
        "file = os.path.join(example_path, f\"{country_code}_tower_examples.geojson\")\n",
        "\n",
        "for country in dirs:\n",
        "    country_code = country_dict[country]\n",
        "    tower_examples_df = gpd.read_file(file)\n",
        "    merged_df = pd.concat([merged_df, tower_examples_df])\n",
        "\n",
        "merged_df = merged_df.drop_duplicates(subset='filename')\n",
        "merged_df.to_file(os.path.join(example_path, \"tower_examples.geojson\"), driver=\"GeoJSON\")\n",
        "display(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hPWfhqch6iP"
      },
      "outputs": [],
      "source": [
        "df = gpd.read_file(os.path.join(os.getcwd(), 'MAXAR_256', 'tower_examples.geojson'))\n",
        "files = os.listdir(os.getcwd() + '/MAXAR_256')\n",
        "\n",
        "df['exists'] = df['filename'].apply(lambda entry: entry in files)\n",
        "\n",
        "print(df['exists'].sum())\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMjkd8SCRuBH"
      },
      "outputs": [],
      "source": [
        "file = os.path.join(base_path, 'MAXAR_256', 'tower_examples.geojson')\n",
        "filter_tiles.filter_images(file, delete_filtered=True, black_point=50, \n",
        "                           dark_threshold=.5, white_point=150, cloudy_threshold=.45, \n",
        "                           blurry_threshold=.65)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj7yac0cR1bm"
      },
      "outputs": [],
      "source": [
        "filter_tiles.verify_df_img('tower_examples_clean.geojson')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yjFHeL6YR4_z"
      },
      "source": [
        "#### Train-Test Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBoK0FeMR4Pv"
      },
      "outputs": [],
      "source": [
        "def split_data(gdf_path, train_ratio=0.8, seed=202):\n",
        "    df = gpd.read_file(gdf_path)\n",
        "    df_dir = os.path.dirname(os.path.abspath(gdf_path)) +'/'\n",
        "    train = df.sample(frac=train_ratio, random_state=seed) #random state is a seed value\n",
        "    \n",
        "    val = df.drop(train.index)\n",
        "    val = val.sample(frac=1, random_state=seed)\n",
        "    train.to_file(df_dir + \"tower_examples_train.geojson\", driver=\"GeoJSON\")\n",
        "    val.to_file(df_dir + \"tower_examples_val.geojson\", driver=\"GeoJSON\")\n",
        "\n",
        "    return train, val\n",
        "\n",
        "train,val = split_data(os.path.join(base_path, 'MAXAR_256', \"tower_examples_clean.geojson\"), \n",
        "                                    train_ratio=0.8, seed=202)\n",
        "display(train)\n",
        "display(val)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F58wkBFJSE18"
      },
      "source": [
        "#### Create COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXuQvLzdSENS"
      },
      "outputs": [],
      "source": [
        "train_file = os.path.join(base_path, 'MAXAR_256', 'tower_examples_train.geojson')\n",
        "val_file = os.path.join(base_path, 'MAXAR_256', 'tower_examples_val.geojson')\n",
        "\n",
        "create_coco_format.to_coco(train_file, '_train', height=256, width=256)\n",
        "create_coco_format.to_coco(val_file, '_val', height=256, width=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCyz7aage5dI"
      },
      "outputs": [],
      "source": [
        "# The directory containing the source images\n",
        "data_path = ...\n",
        "# The path to the COCO labels JSON file\n",
        "labels_path = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPAuTG_jfFyV"
      },
      "outputs": [],
      "source": [
        "# Import the dataset\n",
        "import os\n",
        "\n",
        "# The directory containing the source images\n",
        "mode = 'train'\n",
        "data_path = \"dataset_storage_path\"\n",
        "# The path to the COCO labels JSON file\n",
        "labels_path = data_path + '/labels.json'\n",
        "\n",
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_type=fo.types.COCODetectionDataset,\n",
        "    data_path=data_path,\n",
        "    labels_path=labels_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3FYipXXfIT3"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XHAUnlOyGBnY"
      },
      "source": [
        "#### transfer to dataset directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3FphGCVOHCi"
      },
      "outputs": [],
      "source": [
        "from shutil import copyfile\n",
        "\n",
        "settypes = ['train', 'val']\n",
        "\n",
        "for settype in settypes:\n",
        "    origin = os.path.join(base_path, \"MAXAR_256\")\n",
        "    destpath = os.path.join(base_path, 'datasets', 'maxar_'+settype)\n",
        "\n",
        "    print(os.listdir(destpath))\n",
        "    for file in [os.path.join(destpath, f) for f in os.listdir(destpath)]:\n",
        "        os.remove(file)\n",
        "\n",
        "    gdf =  os.path.join(origin, 'tower_examples_'+settype+'.geojson')\n",
        "    gdf = gpd.read_file(gdf)\n",
        "\n",
        "    for i, row in gdf.iterrows():\n",
        "\n",
        "        file = row['filename']\n",
        "        copyfile(os.path.join(origin, file), os.path.join(destpath, file))\n",
        "\n",
        "    copyfile(os.path.join(origin, 'tower_coco_'+settype+'.json'), \n",
        "             os.path.join(destpath, 'labels.json'))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "ffdb0c89cc842367a93be6eb468afe23766ac3749a570c7a1ecbb8f757cdd35d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
